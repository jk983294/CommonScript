# linux IO
http://www.ilinuxkernel.com/files/Linux.IO.stack_v1.0.pdf

### IO引擎
lib库层有很多种函数可以选择，这事实上就是在选择Linux提供的IO引擎。read、write函数是属于sync引擎，除了sync，还有map、psync、vsync、libaio、posixaio等。 sync，psync都属于同步方式，libaio和posixaio属于异步IO。

### VFS虚拟文件系统
VFS诞生的思想是抽象一个通用的文件系统模型，对开发人员提供一组通用的接口。VFS提供的核心数据结构有四个:
* superblock：Linux用来标注具体已安装的文件系统的有关信息
* inode：Linux中的每一个文件都有一个inode，你可以把inode理解为文件的身份证
* file：内存中的文件对象，用来保存进程和磁盘文件的对应关系
* desty：目录项，是路径中的一部分，所有的目录项对象串起来就是一棵Linux下的目录树。

### Page Cache
页高速缓存，是Linux内核使用的主要磁盘高速缓存，是一个纯内存的工作组件，其作用就是来给访问相对比较慢的磁盘来进行访问加速。如果要访问的文件block正好存在于Page Cache内，那么并不会有实际的磁盘IO发生。如果不存在，那么会申请一个新页，发出缺页中断，然后用磁盘读取到的block内容来填充它，下次直接使用。Linux内核使用搜索树来高效管理大量的页面。

如果你有特殊的需求想要绕开Page Cache，只要设置DIRECT_IO就可以了。有两种情况需要绕开：
1. 测试磁盘IO的真实性能
2. 节约使用Page Cache时系统调用陷入到内核态，以及内核内存向用户进程内存拷贝到开销。

### 文件系统
文件系统里最重要的两个概念就是inode和block，每个文件系统还会定义自己的实际操作函数。

### 通用块层
通用块层是一个处理系统中所有块设备IO请求的内核模块。一个叫bio的数据结构来表示一次IO操作请求。

一次bio里对应的IO大小单位是段！每个bio可能会包含多个段。

### IO调度层
当通用块层把IO请求实际发出以后，并不一定会立即被执行。因为调度层会从全局出发，尽量让整体磁盘IO性能最大化。大致的工作方式是让磁头类似电梯那样工作，先往一个方向走，到头再回来，这样磁盘效率会比较高一些。具体的算法有noop，deadline和cfg等
```sh
dmesg | grep -i scheduler
```

# 读文件过程
* lib里的read函数首先进入系统调用sys_read
* 在sys_read再进入VFS里的vfs_read、generic_file_read等函数
* 在vfs里的generic_file_read会判断是否缓存命中，命中则返回
* 若不命中内核在Page Cache里分配一个新页框，发出缺页中断
* 内核向通用块层发起块I/O请求，块设备屏蔽了磁盘、U盘的差异
* 通用块层把用bio代表的I/O请求放到IO请求队列中
* IO调度层通过电梯算法来调度队列中的请求
* 驱动程序向磁盘控制器发出读取命令控制，DMA方式直接填充到Page Cache中的新页框
* 控制器发出中断通知
* 内核将用户需要的1个字节填充到用户内存中
* 然后你的进程被唤醒

如果Page Cache不命中的话，Linux实际进行了多少个字节的磁盘IO。整个IO过程中涉及到了好几个内核组件。而每个组件之间都是采用不同长度的块来管理磁盘数据的。
* Page Cache是以页为单位的，Linux页大小一般是4KB
* 文件系统是以块为单位来管理的。使用dumpe2fs可以查看，一般一个块默认是4KB
* 通用块层是以段为单位来处理磁盘IO请求的，一个段为一个页或者是页的一部分
* IO调度程序通过DMA方式传输N个扇区到内存，扇区一般为512字节
* 硬盘也是采用“扇区”的管理和传输数据的

# 写文件过程
### balance_dirty_pages_ratelimited
绝大部分情况下，都是直接写到Page Cache里就返回了。但在一种情况下，用户进程必须得等待写入完成才可以返回，那就是对balance_dirty_pages_ratelimited的判断如果超出限制了。该函数判断当前脏页是否已经超过脏页上限dirty_bytes、dirty_ratio，超过了就必须得等待
```sh
cat /proc/sys/vm/dirty_bytes
cat /proc/sys/vm/dirty_ratio
```

代码中的写入，其实绝大部分情况都是写入到PageCache中就返回了，这时并没有真正写入磁盘。我们的数据会在如下三个时机下被真正发起写磁盘IO请求：
1. 如果write系统调用时，如果发现PageCache中脏页占比太多，超过了dirty_ratio或dirty_bytes，write就必须等待了。
2. write写到PageCache就已经返回了。worker内核线程异步运行的时候，再次判断脏页占比，如果超过了dirty_background_ratio或dirty_background_bytes，也发起写回请求。
3. 同样write调用写到PageCache已经返回了。worker内核线程异步运行的时候，虽然系统内脏页一直没有超过dirty_background_ratio或dirty_background_bytes，但是脏页在内存中呆的时间超过dirty_expire_centisecs了，也会发起会写。

如果你做的是和钱相关非常重要的业务，必须保证落盘完成才能返回，那么你就可能需要考虑使用fsync

# empty file
* 新建一个空文件需要消耗掉一个inode，用来保存用户、创建时间等元数据。
* 新建一个空文件还需要消耗掉其所在目录的block中一定的空间，这些空间用来保存文件名，inode号等信息
* 空文件不消耗block，空目录一开始就消耗block，那是因为其必须默认带两个目录项"."和".."

一个文件夹消耗磁盘空间:
1. 首先要消耗掉一个inode，256字节
2. 需要消耗其父目录下的一个目录项ext4_dir_entry_2，保存自己inode号，目录名, 文件名越长，在其父目录中消耗的空间也会越大
3. 其下面如果创建文件夹或者文件的话，它就需要在自己的block里ext4_dir_entry_2数组

当你的文件夹下面文件特别多，尤其是文件名也比较长的时候，它会消耗掉非常多的block。当你遍历文件夹的时候，如果Page Cache中没有命中你要访问的block，就会穿透到磁盘上进行实际的IO

多创建一些文件夹就好了，一个目录下别存太多。工程实践中，一般的做法就是通过一级甚至是二级hash把文件散列到多个目录中，把单目录文件数量控制在十万或万以下

# 1 byte file
哪怕是一个字节，其实操作系统也会给你分配4K的

所以，不要在你的系统里维护一大堆的碎文件。文件再小，占用磁盘其实一点都不少！

inode中定义的block数组15, 12个数组直接存block指针，其余的用来做间接索引（EXT2_IND_BLOCK），二级间接索引（EXT2_DIND_BLOCK）和三级索引（EXT2_TIND_BLOCK）

文件大的时候，访问一个block可能得先进行三次的IO，性能略慢，不过有OS层面的页缓存、目录项缓存的加持，也还好

# directory
创建目录的时候，操作系统会在inode位图上寻找尚未使用的inode编号，找到后把inode分配给你。目录会默认分配一个block，所以还需要查询block位图，找到后分配一个block。在block里面，存储的就是文件系统自己定义的目录想数据结构了，例如ext4_dir_entry_2。每一个结构里会保存其下的文件名，文件的inode编号等信息

# 磁盘结构
常见的机械磁盘，分磁盘面、磁道、柱面和扇区

* 磁盘面：磁盘是由一叠磁盘面叠加组合构成，每个磁盘面上都会有一个磁头负责读写。
* 磁道(Track)：每个盘面会围绕圆心划分出多个同心圆圈，每个圆圈叫做一个磁道。
* 柱面(Cylinders)：所有盘片上的同一位置的磁道组成的立体叫做一个柱面。
* 扇区(Sector)：以磁道为单位管理磁盘仍然太大，所以计算机前辈们又把每个磁道划分出了多个扇区

整体磁盘的容量 = 每个扇区的字节数 * 每个柱面扇区数 * 磁盘面数 * 柱面数

```sh
lsblk
fdisk -l /dev/sda
cat /sys/block/sda/queue/physical_block_size
cat /sys/block/sda/queue/logical_block_size
```

### 寻址模式
* 老式的磁盘里，每个磁道数据都是一样的。通过一个CHS地址：柱面地址（Cylinders）、磁头地址（Heads）、扇区地址（Sectors）直接定位到存储数据所在的扇区
* 现代的磁盘，外圈磁道的扇区比内圈磁道多。这种磁盘寻址模式叫做LBA(Logic Block Address, 扇区的逻辑块地址)。磁盘内部会通过磁盘控制器来完成CHS到LBA的转换，进而定位到具体的物理扇区

# latency
单次磁盘IO时间 = 寻道时间 + 旋转延迟 + 存取时间
* 寻道时间, 磁头径向移动来寻找数据所在的磁道。现代磁盘大概在3-15ms，寻道时间大小主要受磁头当前所在位置和目标磁道所在位置相对距离的影响
* 旋转延迟，找到目标磁道后通过盘面旋转，将目标扇区移动到磁头的正下方。现在主流服务器上经常使用的是1W转/分钟的磁盘，每旋转一周所需的时间为60*1000/10000=6ms，故其旋转延迟为（0-6ms）
* 存取时间，向目标扇区读取或者写入数据。这个是电磁操作，所以一般耗时较短，为零点几ms

操作系统通过按磁道对应的柱面划分分区，来降低磁盘IO所花费的的寻道时间 ，最终提高磁盘的读写性能

# RAID
假如有8块256GB的硬盘，那么RAID5方案下的磁盘阵列从用户角度来看可用的存储空间是7*256GB，只“浪费”了一块盘的空间

### RAID卡缓存
现代磁盘本身也基本都带了缓存，硬件开发者们又搞出来了一层“内存”，并且还自己附带一块电池，这就是RAID卡缓存

拿目前服务器端出镜率比较高的H730和H730P来看，他们分别带了1G和2G的缓存卡，并且自带电池。 电池的作用就是当发现主机意外断电的时候，能够快速把缓存中的数据写回到磁盘中去。

对于写入，一般操作系统写到这个RAID卡里就完事了，所以速度快。对于读取也是，只要缓存里有，就不会透传到磁盘的机械轴上

```sh
cat /proc/scsi/scsi
# Model: PERC H730 这块RAID卡带有1G缓存和电池

# 看硬盘阵列
/opt/MegaRAID/MegaCli/MegaCli64  -LDInfo -Lall -aALL  
/opt/MegaRAID/MegaCli/MegaCli64 -PDList -aALL
```
#### Strip Size
条带大小，if配置是128KB。假如有一个512KB的文件，它就会被分成4个条带，每个128KB。这些条带可能会分散在不同的磁盘上，如果一次性读取的话，多个硬盘就可以一起转动机械轴，读取速度就会提高到原来的数倍

# SSD
SSD是由一些电路和黑色的存储颗粒构成。SSD硬盘是基于NAND Flash存储技术的，属于非易失性存储设备。每一个黑色的存储颗粒也叫做一个Die。

每个Die有若干个Plane，每个Plane有若干个Block，每个Block有若干个Page。Page是磁盘进行读写的最小单位，一般为2KB/4KB/8KB/16KB等。每一个Page里，又包含了许多的闪存单元。。现代的闪存单元有多种类型，目前主流的主要分为SLC(1bit)、MLC(2bit)和TLC(3bit)

工业级的盘往往采用的闪存单元是SLC或MLC，而家用的笔记本一般都是TCL，因为便宜

假设某SSD的Page大小是4KB，一个文件是16KB。那么该文件是存在多个黑色的存储颗粒里,这样多个Flash通道的带宽会充分发挥出来，传输速度也更快

# 格式化原理
```sh
mkfs -t ext4 /dev/vdb
```

块大小设置的是4096字节，分析两种应用场景：
1. 假如你的文件系统全部都用来存储1KB以下的小文件，这个时候你的磁盘1/3的空间将会被浪费无法使用。
2. 假如你的文件全都是GB以上的大文件，这个时候你的inode索引节点里就需要直接或间接维护许许多多的block索引号

很明显，以上这两种情况下4096字节的块大小是不合适的。你需要自己根据情况选择自己的块大小进行重新格式化。


平均4个block会有一个inode。再举分析两种应用场景：
1. 假如说我们的文件都是4KB以下的，那么我们的文件系统用到最后出现的情况就是inode全部用光了，还有1/3的block空闲，而且再也没有办法创建新文件了。
2. 假如我们的文件都特别大，每一个文件需要1000个block，最后的情况就是block全部都用光了，但是inode又都空闲下来了，这个时候也是没办法再建文件的。

这些情况下，block和inode的配比也都是不符合你使用的，你需要根据自己的业务重新配置