UNIX编程艺术
200到400之间逻辑行（400-800的物理行）的代码是最佳点，可能的缺陷密度达到最小
正交性：无论你控制的是什么系统，改变每个属性的方法有且只有一个
SPOT（single point of truth）原则:真理的单点性
前端程序宜用自顶向下，底层程序、系统程序宜采用自底向上，以函数或库来收集底层的域原语，这样，当高层设计变化时，底层原语仍然可以重用
实际代码往往是自顶向下和自底向上的综合产物
完美之道，不在无可增加，而在无可删减
OO在GUI、仿真和图形取得的成功，主要原因之一可能是因为在这些领域里很难弄错类型的本体问题。OO显示出某种使程序员陷入过度分层陷阱的倾向
数据文件元格式
DSV（delimiter separated values）\转义符，\\表示\，设计的比CSV好，CSV中如果有逗号用双引号，有双引号又有其他规则，容易出bug
record-jar格式，metadata之后由%%\n分割每条record，record由key value pair组成
让UI沉默只做对了一半，真正的聪明是找到一个办法，可以访问细节，又不让它们太显眼
SNG用于二进制PNG文件与纯文本双向无损转换
pipe，所有子程序并行，等待前一个子程序的输入，产生输出到下一个子程序
Bernstein chaining，每个继发阶段的程序取代了前一阶段的程序，而不是与之并行
pidfile技巧，/var/run下，需要信号的程序把自己的pid写下，其他程序读该文件获得目标pid。pidfile也可用作为隐含的文件锁使用，还可以用来检测是否已经有本程序实例在运行
SIGHUP用于系统deamon重新初始化信号，SIGTERM温和关闭信号，做GC后退出，SIGKILL立即杀死进程，而且本身不能被阻塞或另外处理
AF_INET famaily。地址被解释为主机地址和服务编号对。AF_UNIX(AF_LOCAL) family支持同一台机器上两个进程之间的通信方式（名字被解析为特殊文件的位置，与双向命名管道类似）
使用共享内存和信号量功能可以避免通过网络栈复制数据的开销
IPC由于文本RPC(SOAP)优于二进制RPC(Corba)
把线程、远程调用接口和重量级面向对象设计结合使用特别危险
真实世界里的编程其实就是管理复杂度的问题。能够管理复杂度的工具都是好东西
m4宏处理程序，用于文本转换
awk程序运行时一行一行过滤输入文件，每一行都顺序经过模式/行为对检查，如果模式与行匹配，则执行相关的行为。awk正在逐渐被Perl取代
bc(basic calculator)代数标记法计算器，dc(desk calculator)逆波兰标记法计算器，支持任意精度计算
宏与带副作用的表达式之间的交互作用可能导致不幸的结果，而且难以诊断     #define max(x, y)   x > y ? x : y
配置信息：1. /etc控制文件 2. 系统控制的环境变量 3. 用户主目录中点文件(.**rc) 4. 用户设置的环境变量 5. 启动程序的命令行参数； 后面的覆盖前面的，越后面的配置越local
-D define -i initialize/interactive -I include
接口模式： 1. 配置者/执行者 2. spooler/daemon 3. 驱动/引擎 4. client/server 5. CLI服务器
premature optimization is the root of all evil
最强大的优化技术就是不做优化
最有效的代码优化就是保持代码短小简单。通常，指令加载要比执行花费的时间更多
尽量避免协议的往返。每个要求握手的协议事务都可能从任何连接延迟发展到潜在的严重降速
减少延时策略： 1. 对可以共享开启开销的事务进行批处理 2. 允许事务重叠 3. 缓存
选择需要管理的上下文环境，并且按照边界所允许的最小化方式构建程序。只有实证了其他方法行不通时才写庞大程序
C语言最佳之处是资源效率和接近机器语言。最糟糕的地方是其编程简直就是资源管理的炼狱
C++最佳之处是编译效率以及面向对象和泛型编程的结合。最糟之处是它非常怪异复杂，往往鼓励过分复杂的设计
shell最佳之处在于书写小型脚本非常自然快捷
Perl最强功能是内置的对文本、面向行的数据格式进行模式导向的处理功能。最佳之处是作为强力工具以供大量涉及正则表达式匹配的小型胶合脚本使用。最糟之处在于当程序很大时Perl会变得丑陋刻板，几乎无法维护
Python最佳之处在于它鼓励清晰易读的代码，易学易用，又能够扩展到大型项目。最糟之处在于效率低下、速度缓慢
递归make有害论。 Recursive make considered harmful
make all/ test/ clean/dist/distclean/realclean/install/uninstall
GDB支持C/C++调试
gprof可以处理C/C++的性能profiler
代码重用，man -k something
README  INSTALL/AUTHORS/NEWS/HISTORY/CHANGES/COPYING/LICENSE/FAQ


OS
context switch cost: 1) number of cycles for load & store instructions 2) cold cache, cache missing
process creation:
1) fork, copy the parent PCB into new child PCB, child continues execution at the instruction after fork
2) exec, replace child image, load new program and start from first instruction
init is the parent of all processes in UNIX-based OS, Zygote is the parent of all APP processes in Android OS
IPC
1) communication channel like shared buffer,  benefit is OS manages, exactly the same API, downside is overhead, copy data between user mode and kernel mode
2) shared memory, OS establishes a shared channel and maps it into each process address space, benefit is fast since no kernel data copy overhead (but the shared memory setup is expensive, only when lots of messages, the amortised cost is cheap), downside is coder need to handle the complexity for no common API
thread out performance process: 1) thread share the same address space, it means the cost for allocate address space is only once 2) data passing among thread (usually shared variables) is cheaper than process (IPC)
单核使用thread有优势吗？有用，比如thread1读取disk，thread2可以做计算
Join semantic: child_status = join(child_thread), called by parent, wait for child thread to finish, then retrieve the result of child status
condition variable: 1) wait(mutex, cond), mutex is automatically released and go to wait queue, then re-acquire mutex for critical section 2) signal(cond), wake up one thread on waiting list 3) broadcast(cond), wake up all waiting threads
spurious wake-ups: unnecessary wake up
dead lock avoid: 1) get all locks at one shot 2) get locks in the same order (say A first then B)
multi-threading pattern
1) Boss/Worker, worker thread pool, Boss is the producer to produce task into queue, workers are consumers. locality give better performance, worker could be specialized to one task
2) Pipeline: use thread pool, use shared buffer based communication for passing data across stages
3) layered pattern
thread related data structures:
1) PCB: [1] hard process state: virtual address mapping [2] light process state: signals, sys call args
2) user level thread (ULT): UL thread ID, UL registers, thread stack
3) kernel level thread (KLT): stack, registers
thread management visibility:
1) kernel sees: KLTs, CPUs, KL scheduler
2) UL thread library sees: ULTs, available KLTs(decided by user mode thread / kernel mode thread mapping)
thread pinning: bound one user mode thread to specified kernel mode thread, this improves locality
adaptive mutex: on multi-CPU system, it is better to spin rather than block when the owner of the mutex is running on another CPU and critical section is short
interrupts: events generated externally by hardware(IO, timer), asynchronously
signals: triggered by code, synchronously or asynchronously
interrupts vs signals similarity: 1) unique ID 2) can be masked, per-CPU interrupt mask(when masked, hardware interrupt will not be delivered to CPU), per-process signal mask(when masked, kernel sees mask and will not interrupt corresponding thread)
interrupt can be directed to any CPU that has them enabled. we can designate one core to handle the interrupt so that other core won't get disturbed
type of signals: 1) one shot signal: n signals pending == 1 signal pending, must be explicitly re-enabled 2) real time signals, if n signals raised, then handler is called n times
interrupt handling can be divided two parts: 1) top half, execute in current stack, no need to launch another thread, fast, non-block, min amount of processing 2) bottom half, execute in another thread, arbitrary complexity
linux thread support: 1) execution context abstraction is task 2) native POSIX threads library (NPTL), 1:1 model, 1 ULT : 1 KLT
thread benefits: 1) parallelization, speed up 2) specialization, hot cache 3) efficiency, lower memory requirement and cheaper synchronization 4) hide latency of I/O
performance metrics(measurable quantity):
1) wait time 2) throughput 3) CPU utilization 4) execution time 5) platform efficiency 6) performance per money
7) performance per power 8) percentage of SLA violations 9) client perceived performance 10) average resource usage
11) bandwidth (bytes / time) 12) connection rate (request / time)
Are threads useful? it depends on: 1) metrics 2) workload
event driven model, state machine, single address space, single process, single thread of control, event dispatcher dispatch event event handlers, in event handler, when meet sync, it will give control back to dispatcher and put itself into block queue, helper is process which handle the block I/O operations
multiple thread(MT) VS multiple process(MP) VS event driven model(EDM):
MP, good: simple programming; bad: high memory usage, costly context switch, costly to maintain shared state across process
MT, good: shared address space, shared state, cheap context switch; bad: not simple implementation, require synchronization, OS underlying support for threads
EDM, good: single address space, single flow of control, smaller memory requirement, no context switch, no synchronization, asynchronous I/O; bad: applicability to certain classes of applications, not applicable to arbitrary applications, event routing on multi CPU systems
Shortest Job First(SJF) gives best waiting time metric
priority inversion: temp boost priority of mutex owner, then lower priority again when releasing mutex
time slice: more I/O intensive, less time slice is better, more CPU intensive, more time slice is better
multi-level feedback queue(MLFQ), different policy for different level
linux O(1) scheduler, constant time to select/add task, real time (0-99), time sharing (100-139), user processes, default priority 120, nice value (-20-19), different time slice for different priority, user's program tend to CPU intensive which should be given high CPU time slice
linux CFS(completely fair scheduler), red-black tree, order by "vruntime"(time spent on CPU), select task O(1), add task O(log N)
multi-CPU, each CPU has private caches (L1, L2), the last level cache(LLC) may or may not be shared, DRAM is shared by CPUs
multi-core, each core has privete L1, cores within same CPU share LLC and memory
cache affinity important, hierarchical scheduler architecture, run-queue load balance for different CPUs, also NUMA(non-uniform memory access)-aware scheduling used for support
hyper-threading(hardware multi-threading, chip multi-threading, simultaneous multi-threading), still 1 CPU but with very fast context switch, because multiple hardware supported(several sets of registers)
hyper-threading scheduler use hardware informations, CPI(cycles-per-instruction), memory bound task tend to high CPI, CPU bound task tend to low CPI
memory management hardware support:
1) MMU(memory management unit): translate virtual to physical addresses
2) register: pointer to page table, segment base& limit size
3) cache: TLB(translation lookaside buffer)
4) translation: actual physical address generation
memory allocation challenges: external fragmentation (holes)
linux allocators:
1) Buddy Allocator, used in page allocation, start with 2^x area, subdivide into 2^x chunks and find smallest 2^x chunk that can satisfy request, encounter internal fragmentation because not all object is 2^y size
2) Slab Allocator, categorize different object, allocate slabs for that kind of objects, a slab contains n entry for that objects, no internal or external fragmentation
linux free up physical memory: parameters to tune thresholds, categorize pages into different types, "second chance" variation of LRU


C++模板元编程
编译期无法实现迭代，因此，对于元程序，递归是常用的手段
我们可以通过引入一个额外的间接层来解决任何问题。由于primitve不能有内部嵌套类型，所以引入traits
traits: value_type, reference, pointer, difference_type, iterator_category


网络是怎样连接的
IP包的最大长度为65535字节，再减去IP头部和UDP头部的长度，就是UDP协议所能发送的数据最大长度
内网中可用作私有地址的范围仅限以下这些:
10.0.0.0 ~ 10.255.255.255
172.16.0.0 ~ 172.31.255.255
192.168.0.0 ~ 192.168.255.255
地址转换的基本原理是在转发网络包时对IP头部中的IP地址和端口号进行改写
上网的计算机拥有公有地址,这意味着来自互联网的包可以直接到达计算机,对于直接上网的客户端计算机,我们应该采取安装防火墙软件等防御手段
包过滤方式的防火墙可根据接收方 IP 地址、发送方 IP 地址、接收方端口号、发送方端口号、控制位等信息来判断是否允许某个包通过
新创建的套接字副本必须和原来的等待连接的套接字具有相同的端口号,要确定某个套接字时,不仅使用服务器端套接字对应的端口号,还同时使用客户端的端口号再加上IP地址
服务器上可能存在多个端口号相同的套接字,但客户端的套接字都是对应不同端口号的,因此我们可以通过客户端的端口号来确定服务器上的某个套接字
通过客户端 IP 地址、客户端端口号、服务器 IP 地址、服务器端口号这 4 种信息可以确定某个套接字
指代某个套接字时使用描述符,原因是,在套接字刚刚创建好,还没有建立连接的状态下,这4种信息是不全的。此外,为了指代一个套接字,使用一种信息(描述符)比使用4种信息要简单
